{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layer Perceptron Classifier\n",
    "This notebook implements a MLP classifier to run the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are responsible for feature selection. The major options here are\n",
    "- ngram\n",
    "- lemmatization\n",
    "\n",
    "Only run these two cells when wish to generate new numpy files, otherwise loading generated files will be quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FileReader():\n",
    "    def __init__(self, train, test, tfidf=True, lemmatize=False):\n",
    "        df = pd.read_csv(train)\n",
    "        self.train_text = df['text']\n",
    "        self.train_label = df['label']\n",
    "        df = pd.read_csv(test)\n",
    "        self.test_id = df['id']\n",
    "        self.test_text = df['text']\n",
    "        self.tfidf = tfidf\n",
    "        self.stop_words = set(stopwords.words('english') + list(string.punctuation) \\\n",
    "                             + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'])\n",
    "        if lemmatize:\n",
    "            def tokenizer(text):\n",
    "                def is_noun(tag):\n",
    "                    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "                def is_verb(tag):\n",
    "                    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "                def is_adverb(tag):\n",
    "                    return tag in ['RB', 'RBR', 'RBS']\n",
    "                def is_adjective(tag):\n",
    "                    return tag in ['JJ', 'JJR', 'JJS']\n",
    "                def penn_to_wn(tag):\n",
    "                    if is_adjective(tag):\n",
    "                        return wn.ADJ\n",
    "                    elif is_noun(tag):\n",
    "                        return wn.NOUN\n",
    "                    elif is_adverb(tag):\n",
    "                        return wn.ADV\n",
    "                    elif is_verb(tag):\n",
    "                        return wn.VERB\n",
    "                    return wn.NOUN\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                tokens = (word for word in nltk.word_tokenize(text) if len(word) > 1 and word not in self.stop_words and not word.isnumeric())\n",
    "                token_pos = nltk.pos_tag(tokens)\n",
    "                lemmas = (lemmatizer.lemmatize(item, penn_to_wn(pos)) for item, pos in token_pos)\n",
    "                return lemmas\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "            \n",
    "    def getLabel(self):\n",
    "        return self.train_label\n",
    "    \n",
    "    def getTestId(self):\n",
    "        return self.test_id\n",
    "    \n",
    "    def genMatrix(self, ngram=(1, 1)):\n",
    "        Vectorizer = TfidfVectorizer if self.tfidf else CountVectorizer\n",
    "        v = Vectorizer(stop_words=self.stop_words, ngram_range=ngram, lowercase=True, \\\n",
    "                                   min_df=3, max_df=0.999, use_idf=False, tokenizer=self.tokenizer)\n",
    "        train_data_matrix = v.fit_transform(self.train_text)\n",
    "        vv = Vectorizer(stop_words=self.stop_words, ngram_range=ngram, lowercase=True, \\\n",
    "                                  vocabulary=v.vocabulary_, use_idf=False, tokenizer=self.tokenizer)\n",
    "        test_data_matrix = vv.transform(self.test_text)\n",
    "        return train_data_matrix, test_data_matrix, v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMatrices(ngram, lemmatize):\n",
    "    filename = '{}-{}{}.npy'.format(ngram[0], ngram[1], '-lemmatized' if lemmatize else '')\n",
    "    print('Options: ngram =',ngram, 'lemmatize =', lemmatize, 'filename =', filename)\n",
    "    print('Reading files...')\n",
    "    fr = FileReader('data/train.csv', 'data/test.csv', lemmatize=False)\n",
    "    print('Generating matrices...')\n",
    "    train_data_matrix, test_data_matrix, vocab = fr.genMatrix(ngram=(1,5))\n",
    "    train_label, test_id = fr.getLabel(), fr.getTestId()\n",
    "    print('Train:', train_data_matrix.shape, train_label.shape)\n",
    "    print('Vocab:', len(vocab))\n",
    "    print('Test:', test_data_matrix.shape, test_id.shape)\n",
    "    print('Writing numpy file...')\n",
    "    obj = np.array([train_data_matrix, test_data_matrix, vocab, train_label, test_id])\n",
    "    np.save(filename, obj)\n",
    "    print('Done.')\n",
    "    \n",
    "saveMatrices((1, 5), False)\n",
    "saveMatrices((1, 5), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic commands to cache notebook outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below loads a binary numpy array which contains extracted data matrices and labels. Then, we run randomized serach on MLPClassifier to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (16000, 123770) (16000,)\n",
      "Vocab: 123770\n",
      "Test: (4491, 123770) (4491,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "p = np.load('1-5.npy')\n",
    "train_data_matrix, test_data_matrix, vocab, train_label, test_id = p\n",
    "print('Train:', train_data_matrix.shape, train_label.shape)\n",
    "print('Vocab:', len(vocab))\n",
    "print('Test:', test_data_matrix.shape, test_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here because I have using multicore, the outputs do not generate properly, it outputted 1 here only but there were 30 fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped the cell's code and loaded variables  from file '/home/ec2-user/SageMaker/ipycache/mlp'.]\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Iteration 1, loss = 1.44423627\n",
      "Validation score: 0.429375\n",
      "Iteration 2, loss = 1.36670521\n",
      "Validation score: 0.429375\n",
      "Iteration 3, loss = 1.36011420\n",
      "Validation score: 0.429375\n",
      "Iteration 4, loss = 1.34767835\n",
      "Validation score: 0.429375\n",
      "Iteration 5, loss = 1.32115302\n",
      "Validation score: 0.430625\n",
      "Iteration 6, loss = 1.26329960\n",
      "Validation score: 0.485625\n",
      "Iteration 7, loss = 1.16721417\n",
      "Validation score: 0.518750\n",
      "Iteration 8, loss = 1.06200391\n",
      "Validation score: 0.559375\n",
      "Iteration 9, loss = 0.97503328\n",
      "Validation score: 0.573750\n",
      "Iteration 10, loss = 0.90666436\n",
      "Validation score: 0.605625\n",
      "Iteration 11, loss = 0.85154996\n",
      "Validation score: 0.609375\n",
      "Iteration 12, loss = 0.80479912\n",
      "Validation score: 0.624375\n",
      "Iteration 13, loss = 0.76348115\n",
      "Validation score: 0.616875\n",
      "Iteration 14, loss = 0.72540222\n",
      "Validation score: 0.629375\n",
      "Iteration 15, loss = 0.68824809\n",
      "Validation score: 0.631875\n",
      "Iteration 16, loss = 0.65282149\n",
      "Validation score: 0.636875\n",
      "Iteration 17, loss = 0.61777178\n",
      "Validation score: 0.627500\n",
      "Iteration 18, loss = 0.58471254\n",
      "Validation score: 0.635625\n",
      "Iteration 19, loss = 0.55143334\n",
      "Validation score: 0.629375\n",
      "Iteration 20, loss = 0.52048866\n",
      "Validation score: 0.626875\n",
      "Iteration 21, loss = 0.49259008\n",
      "Validation score: 0.626875\n",
      "Iteration 22, loss = 0.46468810\n",
      "Validation score: 0.626250\n",
      "Iteration 23, loss = 0.43998934\n",
      "Validation score: 0.623750\n",
      "Iteration 24, loss = 0.41659628\n",
      "Validation score: 0.624375\n",
      "Iteration 25, loss = 0.39508704\n",
      "Validation score: 0.620625\n",
      "Iteration 26, loss = 0.37503556\n",
      "Validation score: 0.620625\n",
      "Iteration 27, loss = 0.35640063\n",
      "Validation score: 0.616250\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 277.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 688.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 929.4min finished\n"
     ]
    }
   ],
   "source": [
    "%%cache mlp -d ipycache\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, validation_curve\n",
    "param_dist = {\n",
    "    'alpha': np.logspace(-6, -2, 1e4),\n",
    "    'activation': ['relu', 'logistic'],\n",
    "    'hidden_layer_sizes': [(5,5), (10,10), (5,5,5), (10,10,10), (25, 25), (50, 50)]\n",
    "}\n",
    "\n",
    "# run randomized search\n",
    "clf = RandomizedSearchCV(MLPClassifier(verbose=True, early_stopping=True, n_iter_no_change=10), \n",
    "                         param_distributions=param_dist, n_iter=100, \n",
    "                         cv=5, verbose=2, n_jobs=-1, return_train_score=True)\n",
    "clf.fit(train_data_matrix, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127.225514</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>0.598500</td>\n",
       "      <td>0.878715</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000934753</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.602124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608125</td>\n",
       "      <td>0.930078</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>0.936094</td>\n",
       "      <td>0.608818</td>\n",
       "      <td>0.937197</td>\n",
       "      <td>30.513643</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>0.070316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>695.208497</td>\n",
       "      <td>0.162505</td>\n",
       "      <td>0.597562</td>\n",
       "      <td>0.910736</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000818637</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.585572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.913672</td>\n",
       "      <td>0.610069</td>\n",
       "      <td>0.867755</td>\n",
       "      <td>182.522692</td>\n",
       "      <td>0.100331</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.034717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164.714003</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.501188</td>\n",
       "      <td>0.646401</td>\n",
       "      <td>logistic</td>\n",
       "      <td>3.12212e-06</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>66</td>\n",
       "      <td>0.558089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533438</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>69.692542</td>\n",
       "      <td>0.007079</td>\n",
       "      <td>0.058073</td>\n",
       "      <td>0.180057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.061592</td>\n",
       "      <td>0.051578</td>\n",
       "      <td>0.478500</td>\n",
       "      <td>0.558478</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000984236</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>72</td>\n",
       "      <td>0.545909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.554375</td>\n",
       "      <td>0.765391</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>55.136869</td>\n",
       "      <td>0.027205</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>0.156815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.960558</td>\n",
       "      <td>0.030585</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.772409</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.63577e-05</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 1.6...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.504997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562813</td>\n",
       "      <td>0.879922</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.600781</td>\n",
       "      <td>0.523139</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>48.443197</td>\n",
       "      <td>0.031327</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.110243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2472.536669</td>\n",
       "      <td>0.145224</td>\n",
       "      <td>0.619625</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00349264</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 0.00...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611563</td>\n",
       "      <td>0.823438</td>\n",
       "      <td>0.613437</td>\n",
       "      <td>0.888828</td>\n",
       "      <td>0.628518</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>253.746440</td>\n",
       "      <td>0.053193</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.041160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>213.127925</td>\n",
       "      <td>0.055984</td>\n",
       "      <td>0.567750</td>\n",
       "      <td>0.924329</td>\n",
       "      <td>relu</td>\n",
       "      <td>4.08189e-06</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 4.08...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.574641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.956641</td>\n",
       "      <td>0.567813</td>\n",
       "      <td>0.959766</td>\n",
       "      <td>0.610694</td>\n",
       "      <td>0.940712</td>\n",
       "      <td>96.942896</td>\n",
       "      <td>0.035129</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.059917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>245.093364</td>\n",
       "      <td>0.035093</td>\n",
       "      <td>0.566438</td>\n",
       "      <td>0.907838</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0036104</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>46</td>\n",
       "      <td>0.574641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551875</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.958906</td>\n",
       "      <td>0.598186</td>\n",
       "      <td>0.960631</td>\n",
       "      <td>137.663832</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.023911</td>\n",
       "      <td>0.073562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>138.588773</td>\n",
       "      <td>0.048928</td>\n",
       "      <td>0.563312</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.00167156</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582812</td>\n",
       "      <td>0.737344</td>\n",
       "      <td>0.569063</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.572233</td>\n",
       "      <td>0.954695</td>\n",
       "      <td>60.097220</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.079870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55.444024</td>\n",
       "      <td>0.047222</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00614298</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 0.0061...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>18.156570</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>241.998841</td>\n",
       "      <td>0.061330</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.813717</td>\n",
       "      <td>logistic</td>\n",
       "      <td>2.87108e-06</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 2.87...</td>\n",
       "      <td>43</td>\n",
       "      <td>0.549032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585625</td>\n",
       "      <td>0.928750</td>\n",
       "      <td>0.565625</td>\n",
       "      <td>0.786406</td>\n",
       "      <td>0.570982</td>\n",
       "      <td>0.779253</td>\n",
       "      <td>43.466213</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>93.798403</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.541687</td>\n",
       "      <td>0.869711</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000278641</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...</td>\n",
       "      <td>58</td>\n",
       "      <td>0.509681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560625</td>\n",
       "      <td>0.912891</td>\n",
       "      <td>0.583750</td>\n",
       "      <td>0.956484</td>\n",
       "      <td>0.501876</td>\n",
       "      <td>0.671926</td>\n",
       "      <td>24.953618</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.031157</td>\n",
       "      <td>0.101355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>665.683288</td>\n",
       "      <td>0.170553</td>\n",
       "      <td>0.595812</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>logistic</td>\n",
       "      <td>5.04162e-05</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 5.04...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.597127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.957812</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.921562</td>\n",
       "      <td>0.599124</td>\n",
       "      <td>0.916732</td>\n",
       "      <td>99.245551</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.036451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>221.165648</td>\n",
       "      <td>0.061397</td>\n",
       "      <td>0.558688</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0085348</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.569644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.958906</td>\n",
       "      <td>0.506563</td>\n",
       "      <td>0.951719</td>\n",
       "      <td>0.567230</td>\n",
       "      <td>0.961490</td>\n",
       "      <td>103.472348</td>\n",
       "      <td>0.023373</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>0.012324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>93.205306</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>0.523563</td>\n",
       "      <td>0.821011</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.00118989</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.495003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.955469</td>\n",
       "      <td>0.579688</td>\n",
       "      <td>0.844063</td>\n",
       "      <td>0.543465</td>\n",
       "      <td>0.809014</td>\n",
       "      <td>40.844652</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.052873</td>\n",
       "      <td>0.090872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150.342213</td>\n",
       "      <td>0.027770</td>\n",
       "      <td>0.503563</td>\n",
       "      <td>0.569183</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00645626</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>65</td>\n",
       "      <td>0.607121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.848383</td>\n",
       "      <td>115.355553</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.089406</td>\n",
       "      <td>0.175524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>316.932758</td>\n",
       "      <td>0.111779</td>\n",
       "      <td>0.603688</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000991516</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.622111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605313</td>\n",
       "      <td>0.928359</td>\n",
       "      <td>0.591250</td>\n",
       "      <td>0.960703</td>\n",
       "      <td>0.609131</td>\n",
       "      <td>0.954851</td>\n",
       "      <td>5.902406</td>\n",
       "      <td>0.067710</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>0.014737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>60.447569</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1.51119e-05</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 1.5...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>7.547976</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>778.191719</td>\n",
       "      <td>0.154437</td>\n",
       "      <td>0.611313</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00412631</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.609619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608437</td>\n",
       "      <td>0.852656</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.920481</td>\n",
       "      <td>84.170730</td>\n",
       "      <td>0.075889</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.039840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>81.572096</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.562125</td>\n",
       "      <td>0.904734</td>\n",
       "      <td>relu</td>\n",
       "      <td>4.58423e-06</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 4.5842...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.580262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.860469</td>\n",
       "      <td>0.552813</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.535335</td>\n",
       "      <td>0.951883</td>\n",
       "      <td>30.679785</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.057235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.193113</td>\n",
       "      <td>0.057414</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>0.940327</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.47e-05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>39</td>\n",
       "      <td>0.590256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.954375</td>\n",
       "      <td>0.558438</td>\n",
       "      <td>0.936172</td>\n",
       "      <td>0.585679</td>\n",
       "      <td>0.962584</td>\n",
       "      <td>43.606374</td>\n",
       "      <td>0.022041</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>0.016262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>174.533929</td>\n",
       "      <td>0.046187</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.953781</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000115719</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.561524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564063</td>\n",
       "      <td>0.955859</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.945937</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.960319</td>\n",
       "      <td>62.486242</td>\n",
       "      <td>0.025760</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.004874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>48.718978</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.755789</td>\n",
       "      <td>relu</td>\n",
       "      <td>3.42652e-06</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 3.4...</td>\n",
       "      <td>64</td>\n",
       "      <td>0.526546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.754453</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.950234</td>\n",
       "      <td>0.521263</td>\n",
       "      <td>0.953054</td>\n",
       "      <td>27.162675</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.193219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>46.456414</td>\n",
       "      <td>0.035352</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000170696</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>20.890669</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>133.182500</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.896593</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0014006</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 0.0014...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.567770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575313</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>0.561875</td>\n",
       "      <td>0.952812</td>\n",
       "      <td>0.610381</td>\n",
       "      <td>0.957507</td>\n",
       "      <td>37.337493</td>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.064394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2799.928939</td>\n",
       "      <td>0.204287</td>\n",
       "      <td>0.603375</td>\n",
       "      <td>0.882504</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00159485</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 0.00...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.574641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600938</td>\n",
       "      <td>0.826094</td>\n",
       "      <td>0.607187</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>0.619762</td>\n",
       "      <td>0.832057</td>\n",
       "      <td>643.719311</td>\n",
       "      <td>0.143986</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.054601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2248.146689</td>\n",
       "      <td>0.254678</td>\n",
       "      <td>0.603375</td>\n",
       "      <td>0.894264</td>\n",
       "      <td>logistic</td>\n",
       "      <td>4.25856e-06</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 4.25...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.608682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606563</td>\n",
       "      <td>0.923750</td>\n",
       "      <td>0.589688</td>\n",
       "      <td>0.795547</td>\n",
       "      <td>0.616323</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>312.848832</td>\n",
       "      <td>0.097012</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>0.056529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>69.482884</td>\n",
       "      <td>0.072742</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>9.7656e-06</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 9.7...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>10.868597</td>\n",
       "      <td>0.054653</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>55.768523</td>\n",
       "      <td>0.073660</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00817318</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>12.352199</td>\n",
       "      <td>0.053039</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>464.159121</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.588250</td>\n",
       "      <td>0.947374</td>\n",
       "      <td>logistic</td>\n",
       "      <td>2.80831e-06</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 2.80...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.601811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566562</td>\n",
       "      <td>0.953203</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.958281</td>\n",
       "      <td>0.592245</td>\n",
       "      <td>0.952664</td>\n",
       "      <td>60.227167</td>\n",
       "      <td>0.028162</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.014684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>292.462154</td>\n",
       "      <td>0.050365</td>\n",
       "      <td>0.552125</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>logistic</td>\n",
       "      <td>5.65686e-05</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 5.65...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.827656</td>\n",
       "      <td>0.596562</td>\n",
       "      <td>0.950078</td>\n",
       "      <td>0.609131</td>\n",
       "      <td>0.889549</td>\n",
       "      <td>144.793643</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.063539</td>\n",
       "      <td>0.195610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>153.709751</td>\n",
       "      <td>0.038486</td>\n",
       "      <td>0.567562</td>\n",
       "      <td>0.918074</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.00254883</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 0.0025...</td>\n",
       "      <td>45</td>\n",
       "      <td>0.528420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557187</td>\n",
       "      <td>0.936875</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.957266</td>\n",
       "      <td>0.575360</td>\n",
       "      <td>0.957116</td>\n",
       "      <td>61.007374</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.046141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>162.964182</td>\n",
       "      <td>0.052004</td>\n",
       "      <td>0.584125</td>\n",
       "      <td>0.924330</td>\n",
       "      <td>relu</td>\n",
       "      <td>4.84918e-06</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 4.84...</td>\n",
       "      <td>36</td>\n",
       "      <td>0.573704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566875</td>\n",
       "      <td>0.912578</td>\n",
       "      <td>0.602187</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.593183</td>\n",
       "      <td>0.891579</td>\n",
       "      <td>64.715443</td>\n",
       "      <td>0.024781</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.020839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>799.992280</td>\n",
       "      <td>0.095148</td>\n",
       "      <td>0.609563</td>\n",
       "      <td>0.840594</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00610912</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.606496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598437</td>\n",
       "      <td>0.785703</td>\n",
       "      <td>0.616250</td>\n",
       "      <td>0.917344</td>\n",
       "      <td>0.616635</td>\n",
       "      <td>0.888845</td>\n",
       "      <td>155.869819</td>\n",
       "      <td>0.054925</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>637.787717</td>\n",
       "      <td>0.135387</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.891592</td>\n",
       "      <td>logistic</td>\n",
       "      <td>2.13026e-06</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 2.13...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593437</td>\n",
       "      <td>0.942891</td>\n",
       "      <td>0.587187</td>\n",
       "      <td>0.894297</td>\n",
       "      <td>0.619450</td>\n",
       "      <td>0.862834</td>\n",
       "      <td>151.892401</td>\n",
       "      <td>0.069907</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>0.059696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>476.690173</td>\n",
       "      <td>0.104729</td>\n",
       "      <td>0.599187</td>\n",
       "      <td>0.933828</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000933893</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.610868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.869375</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.946719</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.955710</td>\n",
       "      <td>304.786059</td>\n",
       "      <td>0.073648</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>0.033335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>49.477076</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>6.83256e-05</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 6.8325...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>18.190860</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>298.175052</td>\n",
       "      <td>0.050657</td>\n",
       "      <td>0.556562</td>\n",
       "      <td>0.772620</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.0029211</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577812</td>\n",
       "      <td>0.853750</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>133.703598</td>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.063605</td>\n",
       "      <td>0.177770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>156.530841</td>\n",
       "      <td>0.037888</td>\n",
       "      <td>0.584875</td>\n",
       "      <td>0.946250</td>\n",
       "      <td>relu</td>\n",
       "      <td>5.77665e-06</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 5.77...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.598376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.959844</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.946328</td>\n",
       "      <td>0.603502</td>\n",
       "      <td>0.940947</td>\n",
       "      <td>58.983280</td>\n",
       "      <td>0.027766</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.012037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>104.640233</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.585375</td>\n",
       "      <td>0.904839</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.00311565</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 0.0031...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.588382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577812</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.600313</td>\n",
       "      <td>0.882344</td>\n",
       "      <td>0.585679</td>\n",
       "      <td>0.956882</td>\n",
       "      <td>66.336592</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.057552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2391.009165</td>\n",
       "      <td>0.300688</td>\n",
       "      <td>0.603938</td>\n",
       "      <td>0.915966</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000196893</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 0.00...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.613991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600938</td>\n",
       "      <td>0.919844</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.932734</td>\n",
       "      <td>0.610694</td>\n",
       "      <td>0.942587</td>\n",
       "      <td>142.418892</td>\n",
       "      <td>0.138001</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.032823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>340.470006</td>\n",
       "      <td>0.100224</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.895421</td>\n",
       "      <td>relu</td>\n",
       "      <td>3.68941e-05</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 3.68...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.613367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594688</td>\n",
       "      <td>0.882734</td>\n",
       "      <td>0.610313</td>\n",
       "      <td>0.850156</td>\n",
       "      <td>0.619762</td>\n",
       "      <td>0.933604</td>\n",
       "      <td>24.100138</td>\n",
       "      <td>0.078621</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.028473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>60.703812</td>\n",
       "      <td>0.033161</td>\n",
       "      <td>0.450937</td>\n",
       "      <td>0.488319</td>\n",
       "      <td>logistic</td>\n",
       "      <td>2.09765e-05</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 2.0976...</td>\n",
       "      <td>80</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.532208</td>\n",
       "      <td>0.718794</td>\n",
       "      <td>30.077390</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.040619</td>\n",
       "      <td>0.115237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>221.668844</td>\n",
       "      <td>0.086289</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>0.614017</td>\n",
       "      <td>logistic</td>\n",
       "      <td>6.99817e-05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>70</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.555972</td>\n",
       "      <td>0.889002</td>\n",
       "      <td>146.823350</td>\n",
       "      <td>0.032910</td>\n",
       "      <td>0.067018</td>\n",
       "      <td>0.224517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>126.876131</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>0.584625</td>\n",
       "      <td>0.884338</td>\n",
       "      <td>relu</td>\n",
       "      <td>2.8012e-05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.573392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555625</td>\n",
       "      <td>0.956641</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.842109</td>\n",
       "      <td>0.601939</td>\n",
       "      <td>0.921809</td>\n",
       "      <td>70.662056</td>\n",
       "      <td>0.022890</td>\n",
       "      <td>0.018260</td>\n",
       "      <td>0.081161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85.588975</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>0.464687</td>\n",
       "      <td>0.692023</td>\n",
       "      <td>relu</td>\n",
       "      <td>2.49885e-05</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 2.4988...</td>\n",
       "      <td>75</td>\n",
       "      <td>0.503123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566250</td>\n",
       "      <td>0.842422</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>0.073672</td>\n",
       "      <td>0.580050</td>\n",
       "      <td>0.921497</td>\n",
       "      <td>55.570979</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.198151</td>\n",
       "      <td>0.323998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>64.914128</td>\n",
       "      <td>0.063936</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.000615856</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>13.843488</td>\n",
       "      <td>0.060909</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>257.996489</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>0.532875</td>\n",
       "      <td>0.702466</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00152728</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.582761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.613196</td>\n",
       "      <td>0.929698</td>\n",
       "      <td>161.809988</td>\n",
       "      <td>0.022482</td>\n",
       "      <td>0.084107</td>\n",
       "      <td>0.223333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>306.318474</td>\n",
       "      <td>0.042275</td>\n",
       "      <td>0.601313</td>\n",
       "      <td>0.944609</td>\n",
       "      <td>relu</td>\n",
       "      <td>3.4686e-05</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 3.46...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.606808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.903672</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.957266</td>\n",
       "      <td>0.607880</td>\n",
       "      <td>0.960865</td>\n",
       "      <td>15.632681</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.021494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>48.138737</td>\n",
       "      <td>0.045998</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.00194594</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 0.0019...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>18.887209</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>267.336425</td>\n",
       "      <td>0.050288</td>\n",
       "      <td>0.538625</td>\n",
       "      <td>0.786949</td>\n",
       "      <td>logistic</td>\n",
       "      <td>2.56414e-05</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 2.56...</td>\n",
       "      <td>59</td>\n",
       "      <td>0.537477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575937</td>\n",
       "      <td>0.960625</td>\n",
       "      <td>0.558750</td>\n",
       "      <td>0.914844</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.882909</td>\n",
       "      <td>93.491267</td>\n",
       "      <td>0.028371</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.192013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>356.404337</td>\n",
       "      <td>0.203285</td>\n",
       "      <td>0.602688</td>\n",
       "      <td>0.930578</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000574746</td>\n",
       "      <td>(25, 25)</td>\n",
       "      <td>{'hidden_layer_sizes': (25, 25), 'alpha': 0.00...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.595253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.590313</td>\n",
       "      <td>0.959844</td>\n",
       "      <td>0.613508</td>\n",
       "      <td>0.949227</td>\n",
       "      <td>64.780674</td>\n",
       "      <td>0.156146</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>0.028271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>38.959577</td>\n",
       "      <td>0.035351</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>4.61813e-06</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 4.6...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>17.383841</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2468.746955</td>\n",
       "      <td>0.272526</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.915593</td>\n",
       "      <td>logistic</td>\n",
       "      <td>6.19697e-05</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 6.19...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.592130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594063</td>\n",
       "      <td>0.949375</td>\n",
       "      <td>0.593437</td>\n",
       "      <td>0.944297</td>\n",
       "      <td>0.618199</td>\n",
       "      <td>0.898297</td>\n",
       "      <td>180.178951</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>0.026139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1307.212156</td>\n",
       "      <td>0.182041</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.929389</td>\n",
       "      <td>relu</td>\n",
       "      <td>7.67871e-06</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>{'hidden_layer_sizes': (50, 50), 'alpha': 7.67...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.595878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607500</td>\n",
       "      <td>0.928594</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>0.961719</td>\n",
       "      <td>0.621639</td>\n",
       "      <td>0.955398</td>\n",
       "      <td>38.907424</td>\n",
       "      <td>0.109475</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>0.026405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>55.400947</td>\n",
       "      <td>0.033440</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1.38202e-05</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 1.3820...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>20.294627</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>154.270943</td>\n",
       "      <td>0.039312</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.890921</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.00205084</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10, 10), 'alpha': ...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.587758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508750</td>\n",
       "      <td>0.749141</td>\n",
       "      <td>0.551562</td>\n",
       "      <td>0.929297</td>\n",
       "      <td>0.567542</td>\n",
       "      <td>0.950789</td>\n",
       "      <td>68.242575</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.026408</td>\n",
       "      <td>0.072307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>41.316878</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.0010802</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>30.204151</td>\n",
       "      <td>0.048060</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>174.170762</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.584187</td>\n",
       "      <td>0.943781</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.000763288</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>{'hidden_layer_sizes': (10, 10), 'alpha': 0.00...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.568082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585313</td>\n",
       "      <td>0.956719</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.882109</td>\n",
       "      <td>0.595685</td>\n",
       "      <td>0.957272</td>\n",
       "      <td>78.575674</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13.725837</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>0.430688</td>\n",
       "      <td>logistic</td>\n",
       "      <td>6.64022e-05</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>{'hidden_layer_sizes': (5, 5), 'alpha': 6.6402...</td>\n",
       "      <td>81</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.430636</td>\n",
       "      <td>1.962939</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      127.225514         0.016721         0.598500          0.878715   \n",
       "1      695.208497         0.162505         0.597562          0.910736   \n",
       "2      164.714003         0.070489         0.501188          0.646401   \n",
       "3      148.061592         0.051578         0.478500          0.558478   \n",
       "4       59.960558         0.030585         0.521687          0.772409   \n",
       "5     2472.536669         0.145224         0.619625          0.815686   \n",
       "6      213.127925         0.055984         0.567750          0.924329   \n",
       "7      245.093364         0.035093         0.566438          0.907838   \n",
       "8      138.588773         0.048928         0.563312          0.886688   \n",
       "9       55.444024         0.047222         0.430688          0.430688   \n",
       "10     241.998841         0.061330         0.568500          0.813717   \n",
       "11      93.798403         0.037999         0.541687          0.869711   \n",
       "12     665.683288         0.170553         0.595812          0.912404   \n",
       "13     221.165648         0.061397         0.558688          0.947858   \n",
       "14      93.205306         0.048792         0.523563          0.821011   \n",
       "15     150.342213         0.027770         0.503563          0.569183   \n",
       "16     316.932758         0.111779         0.603688          0.946655   \n",
       "17      60.447569         0.040541         0.430688          0.430688   \n",
       "18     778.191719         0.154437         0.611313          0.863184   \n",
       "19      81.572096         0.015328         0.562125          0.904734   \n",
       "20     160.193113         0.057414         0.579500          0.940327   \n",
       "21     174.533929         0.046187         0.582000          0.953781   \n",
       "22      48.718978         0.019956         0.515625          0.755789   \n",
       "23      46.456414         0.035352         0.430688          0.430688   \n",
       "24     133.182500         0.022004         0.577000          0.896593   \n",
       "25    2799.928939         0.204287         0.603375          0.882504   \n",
       "26    2248.146689         0.254678         0.603375          0.894264   \n",
       "27      69.482884         0.072742         0.430688          0.430688   \n",
       "28      55.768523         0.073660         0.430688          0.430688   \n",
       "29     464.159121         0.039326         0.588250          0.947374   \n",
       "..            ...              ...              ...               ...   \n",
       "70     292.462154         0.050365         0.552125          0.810704   \n",
       "71     153.709751         0.038486         0.567562          0.918074   \n",
       "72     162.964182         0.052004         0.584125          0.924330   \n",
       "73     799.992280         0.095148         0.609563          0.840594   \n",
       "74     637.787717         0.135387         0.597500          0.891592   \n",
       "75     476.690173         0.104729         0.599187          0.933828   \n",
       "76      49.477076         0.034343         0.430688          0.430688   \n",
       "77     298.175052         0.050657         0.556562          0.772620   \n",
       "78     156.530841         0.037888         0.584875          0.946250   \n",
       "79     104.640233         0.035782         0.585375          0.904839   \n",
       "80    2391.009165         0.300688         0.603938          0.915966   \n",
       "81     340.470006         0.100224         0.610000          0.895421   \n",
       "82      60.703812         0.033161         0.450937          0.488319   \n",
       "83     221.668844         0.086289         0.485063          0.614017   \n",
       "84     126.876131         0.026739         0.584625          0.884338   \n",
       "85      85.588975         0.027892         0.464687          0.692023   \n",
       "86      64.914128         0.063936         0.430688          0.430688   \n",
       "87     257.996489         0.027423         0.532875          0.702466   \n",
       "88     306.318474         0.042275         0.601313          0.944609   \n",
       "89      48.138737         0.045998         0.430688          0.430688   \n",
       "90     267.336425         0.050288         0.538625          0.786949   \n",
       "91     356.404337         0.203285         0.602688          0.930578   \n",
       "92      38.959577         0.035351         0.430688          0.430688   \n",
       "93    2468.746955         0.272526         0.597500          0.915593   \n",
       "94    1307.212156         0.182041         0.603000          0.929389   \n",
       "95      55.400947         0.033440         0.430688          0.430688   \n",
       "96     154.270943         0.039312         0.556250          0.890921   \n",
       "97      41.316878         0.048457         0.430688          0.430688   \n",
       "98     174.170762         0.034164         0.584187          0.943781   \n",
       "99      13.725837         0.009356         0.430688          0.430688   \n",
       "\n",
       "   param_activation  param_alpha param_hidden_layer_sizes  \\\n",
       "0          logistic  0.000934753                 (10, 10)   \n",
       "1          logistic  0.000818637                 (25, 25)   \n",
       "2          logistic  3.12212e-06             (10, 10, 10)   \n",
       "3          logistic  0.000984236             (10, 10, 10)   \n",
       "4              relu  1.63577e-05                (5, 5, 5)   \n",
       "5          logistic   0.00349264                 (50, 50)   \n",
       "6              relu  4.08189e-06                 (10, 10)   \n",
       "7              relu    0.0036104             (10, 10, 10)   \n",
       "8              relu   0.00167156             (10, 10, 10)   \n",
       "9          logistic   0.00614298                   (5, 5)   \n",
       "10         logistic  2.87108e-06                 (10, 10)   \n",
       "11             relu  0.000278641                (5, 5, 5)   \n",
       "12         logistic  5.04162e-05                 (25, 25)   \n",
       "13             relu    0.0085348             (10, 10, 10)   \n",
       "14             relu   0.00118989                (5, 5, 5)   \n",
       "15         logistic   0.00645626                 (10, 10)   \n",
       "16             relu  0.000991516                 (25, 25)   \n",
       "17         logistic  1.51119e-05                (5, 5, 5)   \n",
       "18         logistic   0.00412631                 (25, 25)   \n",
       "19             relu  4.58423e-06                   (5, 5)   \n",
       "20             relu     1.47e-05             (10, 10, 10)   \n",
       "21             relu  0.000115719                 (10, 10)   \n",
       "22             relu  3.42652e-06                (5, 5, 5)   \n",
       "23         logistic  0.000170696                (5, 5, 5)   \n",
       "24             relu    0.0014006                   (5, 5)   \n",
       "25         logistic   0.00159485                 (50, 50)   \n",
       "26         logistic  4.25856e-06                 (50, 50)   \n",
       "27         logistic   9.7656e-06                (5, 5, 5)   \n",
       "28         logistic   0.00817318             (10, 10, 10)   \n",
       "29         logistic  2.80831e-06                 (10, 10)   \n",
       "..              ...          ...                      ...   \n",
       "70         logistic  5.65686e-05                 (10, 10)   \n",
       "71             relu   0.00254883                   (5, 5)   \n",
       "72             relu  4.84918e-06                 (10, 10)   \n",
       "73         logistic   0.00610912                 (25, 25)   \n",
       "74         logistic  2.13026e-06                 (25, 25)   \n",
       "75             relu  0.000933893                 (25, 25)   \n",
       "76         logistic  6.83256e-05                   (5, 5)   \n",
       "77         logistic    0.0029211                 (10, 10)   \n",
       "78             relu  5.77665e-06                 (10, 10)   \n",
       "79             relu   0.00311565                   (5, 5)   \n",
       "80         logistic  0.000196893                 (50, 50)   \n",
       "81             relu  3.68941e-05                 (25, 25)   \n",
       "82         logistic  2.09765e-05                   (5, 5)   \n",
       "83         logistic  6.99817e-05             (10, 10, 10)   \n",
       "84             relu   2.8012e-05             (10, 10, 10)   \n",
       "85             relu  2.49885e-05                   (5, 5)   \n",
       "86         logistic  0.000615856                (5, 5, 5)   \n",
       "87         logistic   0.00152728                 (10, 10)   \n",
       "88             relu   3.4686e-05                 (25, 25)   \n",
       "89         logistic   0.00194594                   (5, 5)   \n",
       "90         logistic  2.56414e-05                 (10, 10)   \n",
       "91             relu  0.000574746                 (25, 25)   \n",
       "92         logistic  4.61813e-06                (5, 5, 5)   \n",
       "93         logistic  6.19697e-05                 (50, 50)   \n",
       "94             relu  7.67871e-06                 (50, 50)   \n",
       "95         logistic  1.38202e-05                   (5, 5)   \n",
       "96             relu   0.00205084             (10, 10, 10)   \n",
       "97         logistic    0.0010802                (5, 5, 5)   \n",
       "98             relu  0.000763288                 (10, 10)   \n",
       "99         logistic  6.64022e-05                   (5, 5)   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               23   \n",
       "1   {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...               24   \n",
       "2   {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               66   \n",
       "3   {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               72   \n",
       "4   {'hidden_layer_sizes': (5, 5, 5), 'alpha': 1.6...               63   \n",
       "5   {'hidden_layer_sizes': (50, 50), 'alpha': 0.00...                1   \n",
       "6   {'hidden_layer_sizes': (10, 10), 'alpha': 4.08...               44   \n",
       "7   {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               46   \n",
       "8   {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               47   \n",
       "9   {'hidden_layer_sizes': (5, 5), 'alpha': 0.0061...               81   \n",
       "10  {'hidden_layer_sizes': (10, 10), 'alpha': 2.87...               43   \n",
       "11  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...               58   \n",
       "12  {'hidden_layer_sizes': (25, 25), 'alpha': 5.04...               27   \n",
       "13  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               50   \n",
       "14  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...               62   \n",
       "15  {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               65   \n",
       "16  {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...               12   \n",
       "17  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 1.5...               81   \n",
       "18  {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...                5   \n",
       "19  {'hidden_layer_sizes': (5, 5), 'alpha': 4.5842...               48   \n",
       "20  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               39   \n",
       "21  {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               37   \n",
       "22  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 3.4...               64   \n",
       "23  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...               81   \n",
       "24  {'hidden_layer_sizes': (5, 5), 'alpha': 0.0014...               40   \n",
       "25  {'hidden_layer_sizes': (50, 50), 'alpha': 0.00...               14   \n",
       "26  {'hidden_layer_sizes': (50, 50), 'alpha': 4.25...               14   \n",
       "27  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 9.7...               81   \n",
       "28  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               81   \n",
       "29  {'hidden_layer_sizes': (10, 10), 'alpha': 2.80...               29   \n",
       "..                                                ...              ...   \n",
       "70  {'hidden_layer_sizes': (10, 10), 'alpha': 5.65...               54   \n",
       "71  {'hidden_layer_sizes': (5, 5), 'alpha': 0.0025...               45   \n",
       "72  {'hidden_layer_sizes': (10, 10), 'alpha': 4.84...               36   \n",
       "73  {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...                8   \n",
       "74  {'hidden_layer_sizes': (25, 25), 'alpha': 2.13...               25   \n",
       "75  {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...               22   \n",
       "76  {'hidden_layer_sizes': (5, 5), 'alpha': 6.8325...               81   \n",
       "77  {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               51   \n",
       "78  {'hidden_layer_sizes': (10, 10), 'alpha': 5.77...               33   \n",
       "79  {'hidden_layer_sizes': (5, 5), 'alpha': 0.0031...               32   \n",
       "80  {'hidden_layer_sizes': (50, 50), 'alpha': 0.00...               11   \n",
       "81  {'hidden_layer_sizes': (25, 25), 'alpha': 3.68...                7   \n",
       "82  {'hidden_layer_sizes': (5, 5), 'alpha': 2.0976...               80   \n",
       "83  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               70   \n",
       "84  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               34   \n",
       "85  {'hidden_layer_sizes': (5, 5), 'alpha': 2.4988...               75   \n",
       "86  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...               81   \n",
       "87  {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               60   \n",
       "88  {'hidden_layer_sizes': (25, 25), 'alpha': 3.46...               20   \n",
       "89  {'hidden_layer_sizes': (5, 5), 'alpha': 0.0019...               81   \n",
       "90  {'hidden_layer_sizes': (10, 10), 'alpha': 2.56...               59   \n",
       "91  {'hidden_layer_sizes': (25, 25), 'alpha': 0.00...               19   \n",
       "92  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 4.6...               81   \n",
       "93  {'hidden_layer_sizes': (50, 50), 'alpha': 6.19...               25   \n",
       "94  {'hidden_layer_sizes': (50, 50), 'alpha': 7.67...               17   \n",
       "95  {'hidden_layer_sizes': (5, 5), 'alpha': 1.3820...               81   \n",
       "96  {'hidden_layer_sizes': (10, 10, 10), 'alpha': ...               52   \n",
       "97  {'hidden_layer_sizes': (5, 5, 5), 'alpha': 0.0...               81   \n",
       "98  {'hidden_layer_sizes': (10, 10), 'alpha': 0.00...               35   \n",
       "99  {'hidden_layer_sizes': (5, 5), 'alpha': 6.6402...               81   \n",
       "\n",
       "    split0_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "0            0.602124       ...                  0.608125            0.930078   \n",
       "1            0.585572       ...                  0.605625            0.877500   \n",
       "2            0.558089       ...                  0.533438            0.765625   \n",
       "3            0.545909       ...                  0.430625            0.430703   \n",
       "4            0.504997       ...                  0.562813            0.879922   \n",
       "5            0.626796       ...                  0.611563            0.823438   \n",
       "6            0.574641       ...                  0.530000            0.956641   \n",
       "7            0.574641       ...                  0.551875            0.890000   \n",
       "8            0.561836       ...                  0.582812            0.737344   \n",
       "9            0.430668       ...                  0.430625            0.430703   \n",
       "10           0.549032       ...                  0.585625            0.928750   \n",
       "11           0.509681       ...                  0.560625            0.912891   \n",
       "12           0.597127       ...                  0.591875            0.957812   \n",
       "13           0.569644       ...                  0.572500            0.958906   \n",
       "14           0.495003       ...                  0.565000            0.955469   \n",
       "15           0.607121       ...                  0.430625            0.430703   \n",
       "16           0.622111       ...                  0.605313            0.928359   \n",
       "17           0.430668       ...                  0.430625            0.430703   \n",
       "18           0.609619       ...                  0.608437            0.852656   \n",
       "19           0.580262       ...                  0.555312            0.860469   \n",
       "20           0.590256       ...                  0.576250            0.954375   \n",
       "21           0.561524       ...                  0.564063            0.955859   \n",
       "22           0.526546       ...                  0.547188            0.754453   \n",
       "23           0.430668       ...                  0.430625            0.430703   \n",
       "24           0.567770       ...                  0.575313            0.811016   \n",
       "25           0.574641       ...                  0.600938            0.826094   \n",
       "26           0.608682       ...                  0.606563            0.923750   \n",
       "27           0.430668       ...                  0.430625            0.430703   \n",
       "28           0.430668       ...                  0.430625            0.430703   \n",
       "29           0.601811       ...                  0.566562            0.953203   \n",
       "..                ...       ...                       ...                 ...   \n",
       "70           0.430668       ...                  0.562500            0.827656   \n",
       "71           0.528420       ...                  0.557187            0.936875   \n",
       "72           0.573704       ...                  0.566875            0.912578   \n",
       "73           0.606496       ...                  0.598437            0.785703   \n",
       "74           0.587133       ...                  0.593437            0.942891   \n",
       "75           0.610868       ...                  0.615000            0.869375   \n",
       "76           0.430668       ...                  0.430625            0.430703   \n",
       "77           0.576202       ...                  0.577812            0.853750   \n",
       "78           0.598376       ...                  0.578750            0.959844   \n",
       "79           0.588382       ...                  0.577812            0.922500   \n",
       "80           0.613991       ...                  0.600938            0.919844   \n",
       "81           0.613367       ...                  0.594688            0.882734   \n",
       "82           0.430668       ...                  0.430625            0.430703   \n",
       "83           0.430668       ...                  0.430625            0.430703   \n",
       "84           0.573392       ...                  0.555625            0.956641   \n",
       "85           0.503123       ...                  0.566250            0.842422   \n",
       "86           0.430668       ...                  0.430625            0.430703   \n",
       "87           0.582761       ...                  0.430625            0.430703   \n",
       "88           0.606808       ...                  0.599375            0.903672   \n",
       "89           0.430668       ...                  0.430625            0.430703   \n",
       "90           0.537477       ...                  0.575937            0.960625   \n",
       "91           0.595253       ...                  0.605625            0.897813   \n",
       "92           0.430668       ...                  0.430625            0.430703   \n",
       "93           0.592130       ...                  0.594063            0.949375   \n",
       "94           0.595878       ...                  0.607500            0.928594   \n",
       "95           0.430668       ...                  0.430625            0.430703   \n",
       "96           0.587758       ...                  0.508750            0.749141   \n",
       "97           0.430668       ...                  0.430625            0.430703   \n",
       "98           0.568082       ...                  0.585313            0.956719   \n",
       "99           0.430668       ...                  0.430625            0.430703   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0            0.595313            0.936094           0.608818   \n",
       "1            0.598750            0.913672           0.610069   \n",
       "2            0.430625            0.430703           0.430894   \n",
       "3            0.554375            0.765391           0.430894   \n",
       "4            0.521875            0.600781           0.523139   \n",
       "5            0.613437            0.888828           0.628518   \n",
       "6            0.567813            0.959766           0.610694   \n",
       "7            0.578750            0.958906           0.598186   \n",
       "8            0.569063            0.891875           0.572233   \n",
       "9            0.430625            0.430703           0.430894   \n",
       "10           0.565625            0.786406           0.570982   \n",
       "11           0.583750            0.956484           0.501876   \n",
       "12           0.601875            0.921562           0.599124   \n",
       "13           0.506563            0.951719           0.567230   \n",
       "14           0.579688            0.844063           0.543465   \n",
       "15           0.430625            0.430703           0.618824   \n",
       "16           0.591250            0.960703           0.609131   \n",
       "17           0.430625            0.430703           0.430894   \n",
       "18           0.605000            0.835000           0.618824   \n",
       "19           0.552813            0.814297           0.535335   \n",
       "20           0.558438            0.936172           0.585679   \n",
       "21           0.597500            0.945937           0.592871   \n",
       "22           0.552500            0.950234           0.521263   \n",
       "23           0.430625            0.430703           0.430894   \n",
       "24           0.561875            0.952812           0.610381   \n",
       "25           0.607187            0.936484           0.619762   \n",
       "26           0.589688            0.795547           0.616323   \n",
       "27           0.430625            0.430703           0.430894   \n",
       "28           0.430625            0.430703           0.430894   \n",
       "29           0.585000            0.958281           0.592245   \n",
       "..                ...                 ...                ...   \n",
       "70           0.596562            0.950078           0.609131   \n",
       "71           0.577500            0.957266           0.575360   \n",
       "72           0.602187            0.950000           0.593183   \n",
       "73           0.616250            0.917344           0.616635   \n",
       "74           0.587187            0.894297           0.619450   \n",
       "75           0.603125            0.946719           0.592871   \n",
       "76           0.430625            0.430703           0.430894   \n",
       "77           0.595625            0.880469           0.430894   \n",
       "78           0.569688            0.946328           0.603502   \n",
       "79           0.600313            0.882344           0.585679   \n",
       "80           0.589375            0.932734           0.610694   \n",
       "81           0.610313            0.850156           0.619762   \n",
       "82           0.430625            0.430703           0.532208   \n",
       "83           0.430625            0.430703           0.555972   \n",
       "84           0.604375            0.842109           0.601939   \n",
       "85           0.073750            0.073672           0.580050   \n",
       "86           0.430625            0.430703           0.430894   \n",
       "87           0.430625            0.430703           0.613196   \n",
       "88           0.598750            0.957266           0.607880   \n",
       "89           0.430625            0.430703           0.430894   \n",
       "90           0.558750            0.914844           0.590369   \n",
       "91           0.590313            0.959844           0.613508   \n",
       "92           0.430625            0.430703           0.430894   \n",
       "93           0.593437            0.944297           0.618199   \n",
       "94           0.595313            0.961719           0.621639   \n",
       "95           0.430625            0.430703           0.430894   \n",
       "96           0.551562            0.929297           0.567542   \n",
       "97           0.430625            0.430703           0.430894   \n",
       "98           0.592500            0.882109           0.595685   \n",
       "99           0.430625            0.430703           0.430894   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.937197     30.513643        0.000613        0.011291   \n",
       "1             0.867755    182.522692        0.100331        0.009609   \n",
       "2             0.430636     69.692542        0.007079        0.058073   \n",
       "3             0.430636     55.136869        0.027205        0.058571   \n",
       "4             0.804952     48.443197        0.031327        0.023028   \n",
       "5             0.804952    253.746440        0.053193        0.006886   \n",
       "6             0.940712     96.942896        0.035129        0.026316   \n",
       "7             0.960631    137.663832        0.019472        0.023911   \n",
       "8             0.954695     60.097220        0.027736        0.017683   \n",
       "9             0.430636     18.156570        0.014108        0.000105   \n",
       "10            0.779253     43.466213        0.023734        0.011786   \n",
       "11            0.671926     24.953618        0.009504        0.031157   \n",
       "12            0.916732     99.245551        0.071450        0.004701   \n",
       "13            0.961490    103.472348        0.023373        0.026286   \n",
       "14            0.809014     40.844652        0.038853        0.052873   \n",
       "15            0.848383    115.355553        0.022668        0.089406   \n",
       "16            0.954851      5.902406        0.067710        0.011809   \n",
       "17            0.430636      7.547976        0.004734        0.000105   \n",
       "18            0.920481     84.170730        0.075889        0.004874   \n",
       "19            0.951883     30.679785        0.012889        0.018932   \n",
       "20            0.962584     43.606374        0.022041        0.011511   \n",
       "21            0.960319     62.486242        0.025760        0.015781   \n",
       "22            0.953054     27.162675        0.013839        0.044118   \n",
       "23            0.430636     20.890669        0.014703        0.000105   \n",
       "24            0.957507     37.337493        0.018007        0.017228   \n",
       "25            0.832057    643.719311        0.143986        0.015723   \n",
       "26            0.932432    312.848832        0.097012        0.009521   \n",
       "27            0.430636     10.868597        0.054653        0.000105   \n",
       "28            0.430636     12.352199        0.053039        0.000105   \n",
       "29            0.952664     60.227167        0.028162        0.012127   \n",
       "..                 ...           ...             ...             ...   \n",
       "70            0.889549    144.793643        0.028159        0.063539   \n",
       "71            0.957116     61.007374        0.020649        0.023716   \n",
       "72            0.891579     64.715443        0.024781        0.012766   \n",
       "73            0.888845    155.869819        0.054925        0.006754   \n",
       "74            0.862834    151.892401        0.069907        0.011998   \n",
       "75            0.955710    304.786059        0.073648        0.014651   \n",
       "76            0.430636     18.190860        0.014365        0.000105   \n",
       "77            0.430636    133.703598        0.028014        0.063605   \n",
       "78            0.940947     58.983280        0.027766        0.013522   \n",
       "79            0.956882     66.336592        0.015298        0.008985   \n",
       "80            0.942587    142.418892        0.138001        0.008584   \n",
       "81            0.933604     24.100138        0.078621        0.008303   \n",
       "82            0.718794     30.077390        0.020421        0.040619   \n",
       "83            0.889002    146.823350        0.032910        0.067018   \n",
       "84            0.921809     70.662056        0.022890        0.018260   \n",
       "85            0.921497     55.570979        0.011777        0.198151   \n",
       "86            0.430636     13.843488        0.060909        0.000105   \n",
       "87            0.929698    161.809988        0.022482        0.084107   \n",
       "88            0.960865     15.632681        0.011427        0.005307   \n",
       "89            0.430636     18.887209        0.006773        0.000105   \n",
       "90            0.882909     93.491267        0.028371        0.056813   \n",
       "91            0.949227     64.780674        0.156146        0.008612   \n",
       "92            0.430636     17.383841        0.014280        0.000105   \n",
       "93            0.898297    180.178951        0.133922        0.010454   \n",
       "94            0.955398     38.907424        0.109475        0.010454   \n",
       "95            0.430636     20.294627        0.007445        0.000105   \n",
       "96            0.950789     68.242575        0.027999        0.026408   \n",
       "97            0.430636     30.204151        0.048060        0.000105   \n",
       "98            0.957272     78.575674        0.025402        0.009851   \n",
       "99            0.430636      1.962939        0.001012        0.000105   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.070316  \n",
       "1          0.034717  \n",
       "2          0.180057  \n",
       "3          0.156815  \n",
       "4          0.110243  \n",
       "5          0.041160  \n",
       "6          0.059917  \n",
       "7          0.073562  \n",
       "8          0.079870  \n",
       "9          0.000026  \n",
       "10         0.068226  \n",
       "11         0.101355  \n",
       "12         0.036451  \n",
       "13         0.012324  \n",
       "14         0.090872  \n",
       "15         0.175524  \n",
       "16         0.014737  \n",
       "17         0.000026  \n",
       "18         0.039840  \n",
       "19         0.057235  \n",
       "20         0.016262  \n",
       "21         0.004874  \n",
       "22         0.193219  \n",
       "23         0.000026  \n",
       "24         0.064394  \n",
       "25         0.054601  \n",
       "26         0.056529  \n",
       "27         0.000026  \n",
       "28         0.000026  \n",
       "29         0.014684  \n",
       "..              ...  \n",
       "70         0.195610  \n",
       "71         0.046141  \n",
       "72         0.020839  \n",
       "73         0.074100  \n",
       "74         0.059696  \n",
       "75         0.033335  \n",
       "76         0.000026  \n",
       "77         0.177770  \n",
       "78         0.012037  \n",
       "79         0.057552  \n",
       "80         0.032823  \n",
       "81         0.028473  \n",
       "82         0.115237  \n",
       "83         0.224517  \n",
       "84         0.081161  \n",
       "85         0.323998  \n",
       "86         0.000026  \n",
       "87         0.223333  \n",
       "88         0.021494  \n",
       "89         0.000026  \n",
       "90         0.192013  \n",
       "91         0.028271  \n",
       "92         0.000026  \n",
       "93         0.026139  \n",
       "94         0.026405  \n",
       "95         0.000026  \n",
       "96         0.072307  \n",
       "97         0.000026  \n",
       "98         0.030900  \n",
       "99         0.000026  \n",
       "\n",
       "[100 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction.\n"
     ]
    }
   ],
   "source": [
    "test_data_pre = clf.predict(test_data_matrix)\n",
    "\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = test_id\n",
    "sub_df[\"pred\"] = test_data_pre\n",
    "sub_df.to_csv(\"submission-mlp_v2.csv\", index=False)\n",
    "print('Saved prediction.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
